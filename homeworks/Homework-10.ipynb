{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR Convolutional Neural Network\n",
    "### CST 463<br>Jose Mijangos <br>Kirk Worley<br>Victoria Zamora\n",
    "\n",
    "## Imported Modules\n",
    "The CIFAR data set is popular for testing new image classification learning algorithms because the images are only 32 by 32 pixels and belong to ten classes, so training on this data set is relatively quick. Like many others have done, we will construct a convolutional neural network that classifies the CIFAR data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "unpickle allows us to open the CIFAR file and access a dictionary containing the data and labels. \n",
    "shuffle_batches returns a random sample from given batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx, :], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The data was retrieved from Dr. Bruns Google Drive. It contains five training batches of 10000 instances, one test batch of 10000, and a ReadMe file describing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'C:/Users/jose/Desktop/cifar-10-batches-py/data_batch_'\n",
    "filenames = [base_path + str(i) for i in range(1, 6)]\n",
    "batches = [unpickle(batch) for batch in filenames]\n",
    "\n",
    "cf_test_batch = unpickle('C:/Users/jose/Desktop/cifar-10-batches-py/test_batch')\n",
    "X_test = cf_test_batch[b'data']\n",
    "y_test = np.array(cf_test_batch[b'labels'])\n",
    "\n",
    "first_done = False\n",
    "for batch in batches:\n",
    "    if not first_done:\n",
    "        X_train = batch[b'data']\n",
    "        y_train = np.array(batch[b'labels'])\n",
    "        first_done = True\n",
    "    else:\n",
    "        X_train = np.vstack((X_train, batch[b'data']))\n",
    "        y_train = np.concatenate((y_train, batch[b'labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Structure\n",
    "Here we define a layout for our convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "conv1_fmaps = 48\n",
    "conv1_ksize = 2\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "pool1_fmaps = conv1_fmaps\n",
    "\n",
    "conv2_fmaps = 96\n",
    "conv2_ksize = 2\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool2_fmaps = conv2_fmaps\n",
    "\n",
    "conv3_fmaps = 192\n",
    "conv3_ksize = 2\n",
    "conv3_stride = 1\n",
    "conv3_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv3_fmaps\n",
    "\n",
    "n_fc1 = 1024\n",
    "n_fc2 = 512\n",
    "n_fc3 = 256\n",
    "n_fc4 = 128\n",
    "n_fc5 = 64\n",
    "\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Phase\n",
    "Now we actually wire the convolutional neural network together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "norm1 = tf.nn.lrn(conv2, 3, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "with tf.name_scope(\"pool1\"):\n",
    "    pool1 = tf.nn.max_pool(norm1, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "conv3 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv3\")\n",
    "\n",
    "conv4 = tf.layers.conv2d(conv3, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv4\")\n",
    "\n",
    "norm2 = tf.nn.lrn(conv4, 3, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "with tf.name_scope(\"pool2\"):\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "conv5 = tf.layers.conv2d(pool2, filters=conv3_fmaps, kernel_size=conv3_ksize,\n",
    "                         strides=conv3_stride, padding=conv3_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv5\")\n",
    "\n",
    "norm5 = tf.nn.lrn(conv5, 3, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(norm5, ksize=[1, 1, 1, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 2 * 2])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.elu, name=\"fc1\", kernel_initializer=he_init)\n",
    "    fc1 = tf.layers.batch_normalization(fc1, True)\n",
    "    fc1_drop = tf.layers.dropout(fc1, 0.3, training=training)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1, n_fc2, activation=tf.nn.elu, name=\"fc2\", kernel_initializer=he_init)\n",
    "    fc2 = tf.layers.batch_normalization(fc2, True)\n",
    "    fc2_drop = tf.layers.dropout(fc2, 0.3, training=training)\n",
    "    \n",
    "    fc3 = tf.layers.dense(fc2, n_fc3, activation=tf.nn.elu, name=\"fc3\", kernel_initializer=he_init)\n",
    "    fc3 = tf.layers.batch_normalization(fc3, True)\n",
    "    fc3_drop = tf.layers.dropout(fc3, 0.3, training=training)\n",
    "    \n",
    "    fc4 = tf.layers.dense(fc3, n_fc4, activation=tf.nn.relu, name=\"fc4\", kernel_initializer=he_init)\n",
    "    fc4 = tf.layers.batch_normalization(fc4, True)\n",
    "    fc4_drop = tf.layers.dropout(fc4, 0.3, training=training)\n",
    "    \n",
    "    fc5 = tf.layers.dense(fc4, n_fc5, activation=tf.nn.relu, name=\"fc5\", kernel_initializer=he_init)\n",
    "    fc5 = tf.layers.batch_normalization(fc5, True)\n",
    "    fc5_drop = tf.layers.dropout(fc5, 0.3, training=training)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc3_drop, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    predict = tf.arg_max(logits, 1)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training and Test Batches\n",
    "The test data is takes up too much space and causes out of memory errors. Hence, we took only a subset of the test batch to be used in the execution phase. During validation, we also took a subset of the training batch to speed up training but since we are presenting our final model we are using all the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in shuffle_batch(X_test, y_test, 5000):\n",
    "    X_test_new, y_test_new = X_batch, y_batch\n",
    "    break\n",
    "\n",
    "for X_batch, y_batch in shuffle_batch(X_train, y_train, 50000):\n",
    "    X_train_new, y_train_new = X_batch, y_batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Phase\n",
    "Now let us see how well this convolutional neural network preforms during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.375\n",
      "0 Test accuracy: 0.3616\n",
      "1 Train accuracy: 0.434\n",
      "1 Test accuracy: 0.4306\n",
      "2 Train accuracy: 0.475\n",
      "2 Test accuracy: 0.468\n",
      "3 Train accuracy: 0.518\n",
      "3 Test accuracy: 0.469\n",
      "4 Train accuracy: 0.496\n",
      "4 Test accuracy: 0.4978\n",
      "5 Train accuracy: 0.572\n",
      "5 Test accuracy: 0.5102\n",
      "6 Train accuracy: 0.557\n",
      "6 Test accuracy: 0.521\n",
      "7 Train accuracy: 0.582\n",
      "7 Test accuracy: 0.5264\n",
      "8 Train accuracy: 0.609\n",
      "8 Test accuracy: 0.5288\n",
      "9 Train accuracy: 0.613\n",
      "9 Test accuracy: 0.5464\n",
      "10 Train accuracy: 0.623\n",
      "10 Test accuracy: 0.5374\n",
      "11 Train accuracy: 0.653\n",
      "11 Test accuracy: 0.5614\n",
      "12 Train accuracy: 0.663\n",
      "12 Test accuracy: 0.5714\n",
      "13 Train accuracy: 0.683\n",
      "13 Test accuracy: 0.573\n",
      "14 Train accuracy: 0.696\n",
      "14 Test accuracy: 0.5676\n",
      "15 Train accuracy: 0.71\n",
      "15 Test accuracy: 0.5794\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "file_writer = tf.summary.FileWriter('logdir', tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 1000\n",
    "\n",
    "trains = []\n",
    "accs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train_new, y_train_new, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train accuracy:\", acc_train)\n",
    "\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test_new, y: y_test_new})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "        trains.append(acc_train)\n",
    "        accs.append(acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./CIFAR_final_model\")\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got nearly 56% accuracy on the test data on the final epoch, which is pretty good considering there are ten classes and the images are low resolution.\n",
    "## Learning Curve for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [i for i in range(n_epochs)]\n",
    "\n",
    "plt.plot(xs, trains, 'g', label='training')\n",
    "plt.plot(xs, accs, 'r', label='validation')\n",
    "plt.title('Training and Test Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this learning curve it appears that our nueral network begins to overfit the training data after the second epoch. Also, the test accuracy plateaus after the eighth epoch. Perhaps more regularization is needed to prevent the model from overfitting.\n",
    "## Model Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./CIFAR_final_model\")\n",
    "    prediction = predict.eval(feed_dict={X: X_test_new, y: y_test_new})\n",
    "    \n",
    "print(prediction[:20])\n",
    "print(y_test_new[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example our model seems to confuse horses and deer as well as tucks and automobiles, which as understandable.\n",
    "\n",
    "## Tuning Attempts\n",
    "To try and tune the model, we did a multitude of things. We began by adding some dense layers close to the output layer of the model, which improved accuracy very slightly, but not very noticeably. Then we began tweaking the activation functions to use ELU, but this did not really offer any significant change.<br>\n",
    "Next, we began adding convolutional layers to the model, which resulted in a decent increase to test accuracy, bringing it up to about .55 percent. We began to tweak the number of filter maps that each convolutional layer was using, which resulted in more consistent accuracies. Before, accuracies could vary wildly between runs of the net, but they were beginning to get more stable.<br>\n",
    "We continued to tweak the layers further, by adjusting kernels and strides, resulting in the model doing fairly well on training data. Then we began to normalize our dense hidden layers with dropout and batch normalization to prevent the model from overfitting the training data. Our final model measures 63% accuracy on training data and 56% accuracy on the test data with early stopping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
