{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jose Mijangos<br>CST 463<br>Oct 17, 2018\n",
    "# Neural Net for Predicting Default on Credit Card Payment\n",
    "Defaulting on credit card debt is very serious because both the credit provider and the client suffer financially as a result. Our motivation is to minimize the cost associated with clients who default by developing a neural network that predicts whether or not a client will default on their credit card payment the following month. If the neural network accurately classifies clients, then the bank can use the neural network's predictions to detect clients at high risk of defaulting before providing a loan or credit increase.\n",
    "## Imported Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects columns from a dataframe\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "\n",
    "# Label encodes catagorical features\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self, columns = None):\n",
    "        self.columns = columns # list of column to encode\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        output = X.copy()        \n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname, col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)        \n",
    "        return output\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The data contains no missing values, has 25 columns, and 25000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"C:/Users/Josemij39/Desktop/default_cc_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Time Series Features\n",
    "In order to get the best performance from our neural network, we derive new features from the time series data present in the data set. The code below fits a linear model to the time series data of each instance. The coefficients of every linear fit are used as new features. We will also use the mean and standard deviation of the clients pay/bill and bill*pay time series data as new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Time Series Features                   \n",
    "bill_amt_subset = data.iloc[:,[12, 13, 14, 15, 16, 17]]\n",
    "pay_amt_subset = data.iloc[:,[18, 19, 20, 21, 22, 23]]\n",
    "\n",
    "# Placeholder for derived features\n",
    "bill_predictors = np.array([])\n",
    "pay_predictors = np.array([])\n",
    "\n",
    "# Gets coefficients of linear bill time series fit\n",
    "for index, row in bill_amt_subset.iterrows():\n",
    "    fit = np.polyfit([i for i in range(1,7)], row, 1)\n",
    "    \n",
    "    if bill_predictors.size == 0:\n",
    "        bill_predictors = fit\n",
    "    else:\n",
    "        bill_predictors = np.vstack([bill_predictors, fit])\n",
    "\n",
    "# Gets coefficients of linear pay time series fit\n",
    "for index, row in pay_amt_subset.iterrows():\n",
    "    fit = np.polyfit([i for i in range(1,7)], row, 1)\n",
    "    \n",
    "    if pay_predictors.size == 0:\n",
    "        pay_predictors = fit\n",
    "    else:\n",
    "        pay_predictors = np.vstack([pay_predictors, fit])\n",
    "        \n",
    "# Concatenate data and derived features\n",
    "bill_amt_coeff = pd.DataFrame(bill_predictors)\n",
    "pay_amt_coeff = pd.DataFrame(pay_predictors)\n",
    "data = pd.concat([data, bill_amt_coeff, pay_amt_coeff], axis=1)\n",
    "\n",
    "# Derive pay/bill features\n",
    "data[\"pay/bill_amt_mean\"] = np.mean(pay_amt_subset.T) / (np.mean(bill_amt_subset.T) + 1)\n",
    "data[\"pay/bill_amt_sd\"] = np.std(pay_amt_subset.T) / (np.std(bill_amt_subset.T) + 1)\n",
    "\n",
    "# Derive bill*pay features\n",
    "data[\"bill*pay_amt_mean\"] = np.mean(pay_amt_subset.T) * np.mean(bill_amt_subset.T)\n",
    "data[\"bill*bill_amt_sd\"] = np.mean(pay_amt_subset.T) * np.std(bill_amt_subset.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "Before we use data for machine learning, we preprocess the data by imputing and scaling numeric features and label encoding catagorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our pipelines need to know which features are numeric and which features are Categorical\n",
    "num_features = data.columns[[1,5,12,13,14,15,16,17,18,19,20,21,22,23,25,26,27,28,29,30,31,32]]\n",
    "cat_features = data.columns[[2,3,4,6,7,8,9,10,11]]\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipeline = Pipeline([\n",
    "  (\"selector\", DataFrameSelector(num_features)),\n",
    "  (\"remove_nas\", SimpleImputer(strategy=\"median\")),\n",
    "  (\"z-scaling\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "  ('selector', DataFrameSelector(cat_features)),\n",
    "  ('labeler', MultiColumnLabelEncoder()),\n",
    "  ('encoder', OneHotEncoder(sparse = False, categories='auto')),\n",
    "])\n",
    "\n",
    "# Transform and store data as X then store class labels as y\n",
    "X = np.concatenate((num_pipeline.fit_transform(data), cat_pipeline.fit_transform(data)), 1)\n",
    "y = data[\"default.payment.next.month\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Machine Learning\n",
    "We split the data 50/50 to ensure a somewhat even distribution of Positive and Negative classes in the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default distribution in training set\n",
      "\tPositives:  2713\n",
      "\tNegatives:  9787 \n",
      "\n",
      "Default distribution in validation set\n",
      "\tPositives:  2801\n",
      "\tNegatives:  9699\n"
     ]
    }
   ],
   "source": [
    "# The neural network will learn from the training set and we will use the validation set to gauge the models performance\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Displayes distribution of default in training set\n",
    "print(\"\\nDefault distribution in training set\")\n",
    "print(\"\\tPositives: \", np.sum(y_train == 1))\n",
    "print(\"\\tNegatives: \", np.sum(y_train == 0), \"\\n\")\n",
    "\n",
    "# Displayes distribution of default in validation set\n",
    "print(\"Default distribution in validation set\")\n",
    "print(\"\\tPositives: \", np.sum(y_valid == 1))\n",
    "print(\"\\tNegatives: \", np.sum(y_valid == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with plain TensorFlow and Batch Normalization\n",
    "We begin by defining the shape of the neural network and placeholders for mini batch gradient descent. We also define a function that creates neuron layers. After constructing the neural network, we define the loss function, a performance measure, and then execute training and validation of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.8656 Val accuracy: 0.85192\n",
      "1 Batch accuracy: 0.9872 Val accuracy: 0.99056\n",
      "2 Batch accuracy: 1.0 Val accuracy: 0.99904\n",
      "3 Batch accuracy: 1.0 Val accuracy: 0.99992\n",
      "4 Batch accuracy: 1.0 Val accuracy: 0.99992\n",
      "5 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "6 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "7 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "8 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "9 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "10 Batch accuracy: 1.0 Val accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input layer one neuron for each feature in X\n",
    "n_inputs = X.shape[1]\n",
    "\n",
    "# Our neural net has ten hidden layers with geometrically decaying number of neurons\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 512\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 256\n",
    "n_hidden5 = 128\n",
    "n_hidden6 = 128\n",
    "n_hidden7 = 64\n",
    "n_hidden8 = 64\n",
    "n_hidden9 = 32\n",
    "n_hidden10 = 32\n",
    "\n",
    "# There are two possible classes so the output layer has two neurons\n",
    "n_outputs = 2\n",
    "\n",
    "# These placeholders will be used to store mini batches from the training set \n",
    "X_hold = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X_hold\")\n",
    "y_hold = tf.placeholder(tf.int32, shape=(None), name=\"y_hold\")\n",
    "\n",
    "# Defines a neuron layer by its input, number of neurons, name, and activation function\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = np.std(y) + 1\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            Z = tf.layers.batch_normalization(Z, True)\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "# Construction of the neural network\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X_hold, n_hidden1, name=\"hidden1\", activation=tf.nn.elu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.elu)\n",
    "    hidden3 = neuron_layer(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.elu)\n",
    "    hidden4 = neuron_layer(hidden3, n_hidden4, name=\"hidden4\", activation=tf.nn.elu)\n",
    "    hidden5 = neuron_layer(hidden4, n_hidden5, name=\"hidden5\", activation=tf.nn.elu)\n",
    "    hidden6 = neuron_layer(hidden5, n_hidden6, name=\"hidden6\", activation=tf.nn.elu)\n",
    "    hidden7 = neuron_layer(hidden6, n_hidden7, name=\"hidden7\", activation=tf.nn.elu)\n",
    "    hidden8 = neuron_layer(hidden7, n_hidden8, name=\"hidden8\", activation=tf.nn.elu)\n",
    "    hidden9 = neuron_layer(hidden8, n_hidden9, name=\"hidden9\", activation=tf.nn.elu)\n",
    "    hidden10 = neuron_layer(hidden9, n_hidden10, name=\"hidden10\", activation=tf.nn.elu)\n",
    "    logits = neuron_layer(hidden10, n_outputs, name=\"outputs\", activation=tf.nn.elu)\n",
    "\n",
    "# Define loss function to train the neural network\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_hold, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# Set up AdamOptimizer and learning rate\n",
    "learning_rate = 0.00115\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Set up performance measure\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y_hold, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs = 11\n",
    "batch_size = 50\n",
    "\n",
    "# Returns mini batches from the training set\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = 10\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Execute training and validation\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X_hold: X_batch, y_hold: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X_hold: X_batch, y_hold: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X_hold: X_valid, y_hold: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly the neural network defined in plain TensorFlow converges to 100% accuracy in less than 100 steps! The use of derived features and proper random initialization of weights must be the cause of the excellent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.Learn Neural Network with He Initialization and Dropout\n",
    "Let us see if we can replicate the performance of the last neural network with a new neural network defined with TF.Learn. The setup is similar to before except we will use tf.layer.dense instead of our own function to create neuron layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.8504 Val accuracy: 0.85128\n",
      "1 Batch accuracy: 0.988 Val accuracy: 0.99168\n",
      "2 Batch accuracy: 1.0 Val accuracy: 0.99968\n",
      "3 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "4 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "5 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "6 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "7 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "8 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "9 Batch accuracy: 1.0 Val accuracy: 1.0\n",
      "10 Batch accuracy: 1.0 Val accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input layer one neuron for each feature in X\n",
    "n_inputs = X.shape[1]\n",
    "\n",
    "# Our neural net has ten hidden layers with geometrically decaying number of neurons\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 512\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 256\n",
    "n_hidden5 = 128\n",
    "n_hidden6 = 128\n",
    "n_hidden7 = 64\n",
    "n_hidden8 = 64\n",
    "n_hidden9 = 32\n",
    "n_hidden10 = 32\n",
    "\n",
    "# There are two possible classes so the output layer has two neurons\n",
    "n_outputs = 2\n",
    "\n",
    "# These placeholders will be used to store mini batches from the training set \n",
    "X_hold = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X_hold\")\n",
    "y_hold = tf.placeholder(tf.int32, shape=(None), name=\"y_hold\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Define dropout rate\n",
    "dropout_rate = 0.5\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "# Construction of the neural network with he initalization and drop out\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(X_hold, n_hidden1, name=\"hidden1\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, name=\"hidden4\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, name=\"hidden5\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden5_drop = tf.layers.dropout(hidden5, dropout_rate, training=training)\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, name=\"hidden6\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden6_drop = tf.layers.dropout(hidden6, dropout_rate, training=training)\n",
    "    hidden7 = tf.layers.dense(hidden6, n_hidden7, name=\"hidden7\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden7_drop = tf.layers.dropout(hidden7, dropout_rate, training=training)\n",
    "    hidden8 = tf.layers.dense(hidden7, n_hidden8, name=\"hidden8\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden8_drop = tf.layers.dropout(hidden8, dropout_rate, training=training)\n",
    "    hidden9 = tf.layers.dense(hidden8, n_hidden9, name=\"hidden9\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden9_drop = tf.layers.dropout(hidden9, dropout_rate, training=training)\n",
    "    hidden10 = tf.layers.dense(hidden9, n_hidden10, name=\"hidden10\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden10_drop = tf.layers.dropout(hidden10, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden10, n_outputs, name=\"outputs\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss function to train the neural network\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_hold, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# Set up AdamOptimizer and learning rate\n",
    "learning_rate = 0.0115\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Set up performance measure\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y_hold, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Intilize TensorFlow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs = 11\n",
    "batch_size = 50\n",
    "\n",
    "# Returns mini batches from the training set\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = 10\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Execute training and validation\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X_hold: X_batch, y_hold: y_batch, training: True})\n",
    "        acc_batch = accuracy.eval(feed_dict={X_hold: X_batch, y_hold: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X_hold: X_valid, y_hold: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eureka! We did it again and in even fewer steps too! As we saw earlier there is a good distribution of clients in the Positive and Negative class within the validation set. So our neural network must be correctly classifying all the clients in order to score 100% accuracy on the validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
